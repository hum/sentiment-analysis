{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples as sample\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "noun, verb = 'NN', \"VB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/hum/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/hum/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/hum/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hum/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/hum/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download data sample\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "# get tokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "# get a lexical db\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# get pos tagger\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# get stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = sample.tokenized('positive_tweets.json')\n",
    "negative_tweets = sample.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sample.tokenized('positive_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 ['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens), tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_token(token: str) -> bool:\n",
    "    if len(token) == 0:\n",
    "        return False\n",
    "    return token not in string.punctuation and token.lower() not in stop_words\n",
    "\n",
    "# normalize a list of tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemma_tokens = []\n",
    "    \n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if not is_noise(token):\n",
    "            continue\n",
    "        if tag.startswith(noun):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith(verb):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "            \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "        if is_valid_token(token):\n",
    "            lemma_tokens.append(lemmatizer.lemmatize(token, pos))\n",
    "    return lemma_tokens\n",
    "\n",
    "# ad hoc function to remove noise from text\n",
    "def is_noise(token, stopwords = ()) -> str:\n",
    "    if 'http://' in token or 'https://' in token:\n",
    "        return ''\n",
    "    \n",
    "    if '@' in token or '_' in token:\n",
    "        return ''\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#FollowFriday', '@France_Inte', '@PKuchly57', '@Milipol_Paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'] \n",
      "\n",
      " ['#FollowFriday', 'top', 'engage', 'member', 'community', 'week', ':)']\n"
     ]
    }
   ],
   "source": [
    "# original vs the dictionary form\n",
    "print(tokens[0], \"\\n\\n\", lemmatize_tokens(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get cleaned tokens\n",
    "pos_tokens = []\n",
    "neg_tokens = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "for tokens in positive_tweets:\n",
    "    pos_tokens.append(lemmatize_tokens(tokens))\n",
    "\n",
    "for tokens in negative_tweets:\n",
    "    neg_tokens.append(lemmatize_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht'] \n",
      "\n",
      " ['Dang', 'rad', '#fanart', ':D']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweets[500], \"\\n\\n\", pos_tokens[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words in a list\n",
    "def get_all_words(token_list):\n",
    "    for tokens in token_list:\n",
    "        for token in tokens:\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pos_words = get_all_words(pos_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':D', 658), ('follow', 337), ('...', 290), ('love', 244), ('day', 235), ('get', 234), ('u', 228), ('like', 220)]\n"
     ]
    }
   ],
   "source": [
    "# most common words\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list to dict\n",
    "def get_dict_from_list(token_list):\n",
    "    for tokens in token_list:\n",
    "        yield dict([token, True] for token in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dicts of positive and negative tweets for the model\n",
    "pos_tokens_dict = get_dict_from_list(pos_tokens)\n",
    "neg_tokens_dict = get_dict_from_list(neg_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "pos_dataset = [(value, \"Positive\") for value in pos_tokens_dict]\n",
    "neg_dataset = [(value, \"Negative\") for value in neg_tokens_dict]\n",
    "\n",
    "dataset = pos_dataset + neg_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9963333333333333\n"
     ]
    }
   ],
   "source": [
    "# use Naive Bayes Classifier\n",
    "# More at: https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "print('Accuracy: ', classify.accuracy(classifier, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2076.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1636.1 : 1.0\n",
      "                     sad = True           Negati : Positi =     33.6 : 1.0\n",
      "                     See = True           Positi : Negati =     26.8 : 1.0\n",
      "                follower = True           Positi : Negati =     23.2 : 1.0\n",
      "                  THANKS = True           Negati : Positi =     23.2 : 1.0\n",
      "                  FOLLOW = True           Negati : Positi =     22.5 : 1.0\n",
      "                    MUCH = True           Negati : Positi =     21.2 : 1.0\n",
      "                     x15 = True           Negati : Positi =     19.1 : 1.0\n",
      "                   Thank = True           Positi : Negati =     18.9 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test some tweets\n",
    "tweets = [\n",
    "    \"Castaways ain't even the best backyardigans song ðŸ˜­\",\n",
    "    \"I don't get men who wouldn't date a girl with an Onlyfans.\",\n",
    "    \"Great start to the day\",\n",
    "    \"i know my worth bro iâ€™m just dumb as hell\",\n",
    "    \"my coworker keeps looking at my boobs lol\",\n",
    "    \"Hereâ€™s a baby elephant loving life.\",\n",
    "    \"Succesfully wasted almost 2 years of our life because of Covid19.\",\n",
    "    \"When you feel depressed remember there are a million cells in your body and all they do is care about you.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative -> Castaways ain't even the best backyardigans song ðŸ˜­\n",
      "Positive -> I don't get men who wouldn't date a girl with an Onlyfans.\n",
      "Positive -> Great start to the day\n",
      "Positive -> i know my worth bro iâ€™m just dumb as hell\n",
      "Negative -> my coworker keeps looking at my boobs lol\n",
      "Positive -> Hereâ€™s a baby elephant loving life.\n",
      "Negative -> Succesfully wasted almost 2 years of our life because of Covid19.\n",
      "Positive -> When you feel depressed remember there are a million cells in your body and all they do is care about you.\n"
     ]
    }
   ],
   "source": [
    "for tweet in tweets:\n",
    "    # normalize tweet and remove noise\n",
    "    tokens = lemmatize_tokens(word_tokenize(tweet))\n",
    "    \n",
    "    # classify\n",
    "    result = classifier.classify((dict([token, True] for token in tokens)))\n",
    "    print(result, '->', tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
